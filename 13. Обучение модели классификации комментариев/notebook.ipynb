{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Подготовка\" data-toc-modified-id=\"Подготовка-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Подготовка</a></span></li><li><span><a href=\"#Обучение\" data-toc-modified-id=\"Обучение-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Обучение</a></span></li><li><span><a href=\"#Выводы\" data-toc-modified-id=\"Выводы-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Выводы</a></span></li><li><span><a href=\"#Чек-лист-проверки\" data-toc-modified-id=\"Чек-лист-проверки-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Чек-лист проверки</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проект для «Викишоп»"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Постройте модель со значением метрики качества *F1* не меньше 0.75. \n",
    "\n",
    "**Инструкция по выполнению проекта**\n",
    "\n",
    "1. Загрузите и подготовьте данные.\n",
    "2. Обучите разные модели. \n",
    "3. Сделайте выводы.\n",
    "\n",
    "Для выполнения проекта применять *BERT* необязательно, но вы можете попробовать.\n",
    "\n",
    "**Описание данных**\n",
    "\n",
    "Данные находятся в файле `toxic_comments.csv`. Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаем все необходимые библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/datasets/toxic_comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    159571 non-null  object\n",
      " 1   toxic   159571 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"\\n\\nCongratulations from me as well, use the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Your vandalism to the Matt Shirvington article...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sorry if the word 'nonsense' was offensive to ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>alignment on this subject and which are contra...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>\"\\nFair use rationale for Image:Wonju.jpg\\n\\nT...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>bbq \\n\\nbe a man and lets discuss it-maybe ove...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Hey... what is it..\\n@ | talk .\\nWhat is it......</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Before you start throwing accusations and warn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Oh, and the girl above started her arguments w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  toxic\n",
       "0   Explanation\\nWhy the edits made under my usern...      0\n",
       "1   D'aww! He matches this background colour I'm s...      0\n",
       "2   Hey man, I'm really not trying to edit war. It...      0\n",
       "3   \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4   You, sir, are my hero. Any chance you remember...      0\n",
       "5   \"\\n\\nCongratulations from me as well, use the ...      0\n",
       "6        COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK      1\n",
       "7   Your vandalism to the Matt Shirvington article...      0\n",
       "8   Sorry if the word 'nonsense' was offensive to ...      0\n",
       "9   alignment on this subject and which are contra...      0\n",
       "10  \"\\nFair use rationale for Image:Wonju.jpg\\n\\nT...      0\n",
       "11  bbq \\n\\nbe a man and lets discuss it-maybe ove...      0\n",
       "12  Hey... what is it..\\n@ | talk .\\nWhat is it......      1\n",
       "13  Before you start throwing accusations and warn...      0\n",
       "14  Oh, and the girl above started her arguments w...      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изучил данные. Обнарурил специальные символы, единичные символы, специальные символы начала строки, префиксы, различное число пробелов (пропусков), заглавные строки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text     0.0\n",
       "toxic    0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    143346\n",
       "1     16225\n",
       "Name: toxic, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['toxic'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Опредилил количество различных комментариев, дальше приведу все тексты к \"нормальному\" виду и проведу лемматизацию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df['text'].values\n",
    "lemmas = []\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "for sen in range(0, len(corpus)):\n",
    "    document = re.sub(r'\\W', ' ', str(corpus[sen]))\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "    document = re.sub(r'^b\\s+', '', document)\n",
    "    document = document.lower()\n",
    "    document = document.split()\n",
    "    document = [stemmer.lemmatize(word) for word in document]\n",
    "    document = ' '.join(document)\n",
    "    \n",
    "    lemmas.append(document)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['explanation why the edits made under my username hardcore metallica fan were reverted they weren vandalism just closure on some gas after voted at new york doll fac and please don remove the template from the talk page since m retired now 89 205 38 27',\n",
       " 'd aww he match this background colour m seemingly stuck with thanks talk 21 51 january 11 2016 utc']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmas[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создам датафрейм с лемматизированными значениями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>lemmas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>explanation why the edits made under my userna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>d aww he match this background colour m seemin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>hey man m really not trying to edit war it jus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>more can make any real suggestion on improveme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>you sir are my hero any chance you remember wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"\\n\\nCongratulations from me as well, use the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>congratulation from me a well use the tool wel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>1</td>\n",
       "      <td>cocksucker before you piss around on my work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Your vandalism to the Matt Shirvington article...</td>\n",
       "      <td>0</td>\n",
       "      <td>your vandalism to the matt shirvington article...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sorry if the word 'nonsense' was offensive to ...</td>\n",
       "      <td>0</td>\n",
       "      <td>sorry if the word nonsense wa offensive to you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>alignment on this subject and which are contra...</td>\n",
       "      <td>0</td>\n",
       "      <td>alignment on this subject and which are contra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>\"\\nFair use rationale for Image:Wonju.jpg\\n\\nT...</td>\n",
       "      <td>0</td>\n",
       "      <td>fair use rationale for image wonju jpg thanks ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>bbq \\n\\nbe a man and lets discuss it-maybe ove...</td>\n",
       "      <td>0</td>\n",
       "      <td>bbq be man and let discus it maybe over the phone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Hey... what is it..\\n@ | talk .\\nWhat is it......</td>\n",
       "      <td>1</td>\n",
       "      <td>hey what is it talk what is it an exclusive gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Before you start throwing accusations and warn...</td>\n",
       "      <td>0</td>\n",
       "      <td>before you start throwing accusation and warni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Oh, and the girl above started her arguments w...</td>\n",
       "      <td>0</td>\n",
       "      <td>oh and the girl above started her argument wit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  toxic  \\\n",
       "0   Explanation\\nWhy the edits made under my usern...      0   \n",
       "1   D'aww! He matches this background colour I'm s...      0   \n",
       "2   Hey man, I'm really not trying to edit war. It...      0   \n",
       "3   \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4   You, sir, are my hero. Any chance you remember...      0   \n",
       "5   \"\\n\\nCongratulations from me as well, use the ...      0   \n",
       "6        COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK      1   \n",
       "7   Your vandalism to the Matt Shirvington article...      0   \n",
       "8   Sorry if the word 'nonsense' was offensive to ...      0   \n",
       "9   alignment on this subject and which are contra...      0   \n",
       "10  \"\\nFair use rationale for Image:Wonju.jpg\\n\\nT...      0   \n",
       "11  bbq \\n\\nbe a man and lets discuss it-maybe ove...      0   \n",
       "12  Hey... what is it..\\n@ | talk .\\nWhat is it......      1   \n",
       "13  Before you start throwing accusations and warn...      0   \n",
       "14  Oh, and the girl above started her arguments w...      0   \n",
       "\n",
       "                                               lemmas  \n",
       "0   explanation why the edits made under my userna...  \n",
       "1   d aww he match this background colour m seemin...  \n",
       "2   hey man m really not trying to edit war it jus...  \n",
       "3   more can make any real suggestion on improveme...  \n",
       "4   you sir are my hero any chance you remember wh...  \n",
       "5   congratulation from me a well use the tool wel...  \n",
       "6        cocksucker before you piss around on my work  \n",
       "7   your vandalism to the matt shirvington article...  \n",
       "8   sorry if the word nonsense wa offensive to you...  \n",
       "9   alignment on this subject and which are contra...  \n",
       "10  fair use rationale for image wonju jpg thanks ...  \n",
       "11  bbq be man and let discus it maybe over the phone  \n",
       "12  hey what is it talk what is it an exclusive gr...  \n",
       "13  before you start throwing accusation and warni...  \n",
       "14  oh and the girl above started her argument wit...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmas_series = pd.Series(lemmas, name = 'lemmas')\n",
    "comments_new = pd.concat([df, lemmas_series], axis = 1)\n",
    "comments_new.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделю выборки на обучающую и валидационную"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_features = comments_new.drop(['text','toxic'], axis=1)\n",
    "comments_target = comments_new['toxic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_rest, y_train, y_rest = train_test_split(comments_features, comments_target, test_size=0.3, random_state=12345)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_rest, y_rest, test_size=0.3, random_state=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(111699, 1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(33510, 1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(14362, 1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(X_train.shape, X_valid.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пропишу NLTK стоп слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(nltk_stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создам тело обучающей, валидационной и трейновой выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_train = X_train['lemmas'].values\n",
    "corpus_valid = X_valid['lemmas'].values\n",
    "corpus_test = X_test['lemmas'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проведу извлечение текствовых признаков через TfidfVectorizer библиотеки scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(max_df=0.7, max_features=6000, min_df=5,\n",
       "                stop_words={'a', 'about', 'above', 'after', 'again', 'against',\n",
       "                            'ain', 'all', 'am', 'an', 'and', 'any', 'are',\n",
       "                            'aren', \"aren't\", 'as', 'at', 'be', 'because',\n",
       "                            'been', 'before', 'being', 'below', 'between',\n",
       "                            'both', 'but', 'by', 'can', 'couldn', \"couldn't\", ...})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idfconverter = TfidfVectorizer(max_features=6000, min_df=5, max_df=0.7, stop_words=stopwords)\n",
    "tf_idfconverter.fit(corpus_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(111699, 6000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(33510, 6000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(14362, 6000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train_tf = tf_idfconverter.transform(corpus_train)\n",
    "X_valid_tf = tf_idfconverter.transform(corpus_valid)\n",
    "X_test_tf = tf_idfconverter.transform(corpus_test)\n",
    "display(X_train_tf.shape, X_valid_tf.shape, X_test_tf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод:**\n",
    "1. Загрузил и подготовил данные\n",
    "2. Пропусков и дубликантов не обнаружил.\n",
    "3. Провел лемматизацию датафрейма.\n",
    "4. Разделил выборки и подготовил их к обучению моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Путем изучения библиотеки scikit-learn искал библиотеки, которые могу использовать.\n",
    "1. Самая популярная в моём представлении, а также наиболее часто используемая в проектах - LogisticRegression;\n",
    "2. Классификация линейных опорных векторов (LinearSVC), поскольку, он обладает большей гибкостью по сравнению с SVC в выборе функций штрафов и потерь и должен лучше масштабироваться для большого количества выборок;\n",
    "3. SGDClassifier, поскольку SGD был успешно применен к крупномасштабным и разреженным задачам машинного обучения, часто возникающим при классификации текста и обработке естественного языка;\n",
    "4. Наивный байесовский классификатор для текстов (MultinomialNB) - является одним из двух классических наивных байесовских вариантов, используемых в текстовой классификации.\n",
    "\n",
    "Поскольку работаю с 4-мя моделями, напишу функцию для расчёта метрики F1 и занесу результаты в таблицу "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=[]\n",
    "def ml_models(models, ft, tt, fv, tv):\n",
    "    model = models\n",
    "    model.fit(ft, tt)\n",
    "    \n",
    "    predictions_valid = model.predict(fv)\n",
    "    print('F1 = {:.2f}'.format(f1_score(tv, predictions_valid)))\n",
    "    return f1_score(tv, predictions_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 = 0.75\n"
     ]
    }
   ],
   "source": [
    "res.append(ml_models(LogisticRegression(random_state=12345), X_train_tf, y_train, X_valid_tf, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 = 0.77\n"
     ]
    }
   ],
   "source": [
    "res.append(ml_models(LinearSVC(random_state=12345), X_train_tf, y_train, X_valid_tf, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 = 0.67\n"
     ]
    }
   ],
   "source": [
    "res.append(ml_models(SGDClassifier(random_state=12345), X_train_tf, y_train, X_valid_tf, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 = 0.68\n"
     ]
    }
   ],
   "source": [
    "res.append(ml_models(MultinomialNB(), X_train_tf, y_train, X_valid_tf, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Результаты расчёта F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.748073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearSVC</th>\n",
       "      <td>0.773965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SGD</th>\n",
       "      <td>0.671075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MultinomialNB</th>\n",
       "      <td>0.677533</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Результаты расчёта F1\n",
       "LogisticRegression               0.748073\n",
       "LinearSVC                        0.773965\n",
       "SGD                              0.671075\n",
       "MultinomialNB                    0.677533"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = pd.DataFrame(data=res, columns=['Результаты расчёта F1']).rename(index={0: 'LogisticRegression', 1: 'LinearSVC', 2: 'SGD', 3: 'MultinomialNB'})\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные записал, лучший результат по метрике F1 показала LinearSVC. Далее следует подобрать оптимальные гиперпараметры для моделей и посмотреть, что получится. Напишу функцию и посмотрю что получится"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "def gridsearch_result(model, parameters, f, t, scorer):\n",
    "    gridsearch = GridSearchCV(estimator=model, param_grid = parameters, refit=True, scoring=scorer, cv=2)\n",
    "    gridsearch = gridsearch.fit(f, t)\n",
    "        \n",
    "    print('Лучшие гиперпараметры: ', gridsearch.best_params_)\n",
    "    return gridsearch.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшие гиперпараметры:  {'C': 10, 'penalty': 'l2', 'random_state': 12345}\n"
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "        'random_state' : [12345],\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10, 50, 100, 200]\n",
    "        }\n",
    "best_LogisticRegression = gridsearch_result(LogisticRegression(), parameters, X_train_tf, y_train, 'f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 = 0.77\n"
     ]
    }
   ],
   "source": [
    "res.append([ml_models(best_LogisticRegression, X_train_tf, y_train, X_valid_tf, y_valid), best_LogisticRegression.get_params()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшие гиперпараметры:  {'C': 1, 'max_iter': 1000, 'random_state': 12345}\n"
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "        'max_iter' : range(1000, 2000, 100),\n",
    "        'C' : [0.1, 1, 10, 100],\n",
    "        'random_state' : [12345]        \n",
    "        }\n",
    "best_LinearSVC = gridsearch_result(LinearSVC(), parameters, X_train_tf, y_train, 'f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 = 0.77\n"
     ]
    }
   ],
   "source": [
    "res.append([ml_models(best_LinearSVC, X_train_tf, y_train, X_valid_tf, y_valid), best_LinearSVC.get_params()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшие гиперпараметры:  {'alpha': 1e-05, 'penalty': 'l1', 'random_state': 12345}\n"
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "        'alpha' : [1, 1e-01, 1e-02, 1e-03, 1e-04, 1e-05, 1e-06],\n",
    "        'penalty' : ['l1','l2'],\n",
    "        'random_state' : [12345]        \n",
    "        }\n",
    "best_SGD = gridsearch_result(SGDClassifier(), parameters, X_train_tf, y_train, 'f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 = 0.78\n"
     ]
    }
   ],
   "source": [
    "res.append([ml_models(best_SGD, X_train_tf, y_train, X_valid_tf, y_valid), best_SGD.get_params()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшие гиперпараметры:  {'alpha': 0.1}\n"
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "        'alpha' : [1e-06, 1e-05, 1e-04, 1e-03, 1e-02, 1e-01, 1],\n",
    "        }\n",
    "best_MNB = gridsearch_result(MultinomialNB(class_prior=None, fit_prior=True), parameters, X_train_tf, y_train, 'f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 = 0.69\n"
     ]
    }
   ],
   "source": [
    "res.append([ml_models(best_MNB, X_train_tf, y_train, X_valid_tf, y_valid), best_MNB.get_params()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1 с гиперпарам.</th>\n",
       "      <th>Гиперпараметры</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.774472</td>\n",
       "      <td>{'C': 10, 'class_weight': None, 'dual': False,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearSVC</th>\n",
       "      <td>0.773965</td>\n",
       "      <td>{'C': 1, 'class_weight': None, 'dual': True, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SGD</th>\n",
       "      <td>0.776551</td>\n",
       "      <td>{'alpha': 1e-05, 'average': False, 'class_weig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MultinomialNB</th>\n",
       "      <td>0.687875</td>\n",
       "      <td>{'alpha': 0.1, 'class_prior': None, 'fit_prior...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    F1 с гиперпарам.  \\\n",
       "LogisticRegression          0.774472   \n",
       "LinearSVC                   0.773965   \n",
       "SGD                         0.776551   \n",
       "MultinomialNB               0.687875   \n",
       "\n",
       "                                                       Гиперпараметры  \n",
       "LogisticRegression  {'C': 10, 'class_weight': None, 'dual': False,...  \n",
       "LinearSVC           {'C': 1, 'class_weight': None, 'dual': True, '...  \n",
       "SGD                 {'alpha': 1e-05, 'average': False, 'class_weig...  \n",
       "MultinomialNB       {'alpha': 0.1, 'class_prior': None, 'fit_prior...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result_gs = pd.DataFrame(data=res, columns=['F1 с гиперпарам.', 'Гиперпараметры']).rename(index={0: 'LogisticRegression', 1: 'LinearSVC', 2: 'SGD', 3: 'MultinomialNB'})\n",
    "display(result_gs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И теперь расчитаю F1 на тесте."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 = 0.76\n"
     ]
    }
   ],
   "source": [
    "res = []\n",
    "res.append(ml_models(best_LogisticRegression, X_train_tf, y_train,  X_test_tf, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 = 0.76\n"
     ]
    }
   ],
   "source": [
    "res.append(ml_models(best_LinearSVC, X_train_tf, y_train,  X_test_tf, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 = 0.76\n"
     ]
    }
   ],
   "source": [
    "res.append(ml_models(best_SGD, X_train_tf, y_train,  X_test_tf, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 = 0.68\n"
     ]
    }
   ],
   "source": [
    "res.append(ml_models(best_MNB, X_train_tf, y_train,  X_test_tf, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1 тестовая</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.762200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearSVC</th>\n",
       "      <td>0.761346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SGD</th>\n",
       "      <td>0.755837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MultinomialNB</th>\n",
       "      <td>0.684142</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    F1 тестовая\n",
       "LogisticRegression     0.762200\n",
       "LinearSVC              0.761346\n",
       "SGD                    0.755837\n",
       "MultinomialNB          0.684142"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result_test = pd.DataFrame(data=res, columns=['F1 тестовая']).rename(index={0: 'LogisticRegression', 1: 'LinearSVC', 2: 'SGD', 3: 'MultinomialNB'})\n",
    "display(result_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_final = pd.concat([result, result_gs, result_test], axis=\"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Результаты расчёта F1</th>\n",
       "      <th>F1 с гиперпарам.</th>\n",
       "      <th>Гиперпараметры</th>\n",
       "      <th>F1 тестовая</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.748073</td>\n",
       "      <td>0.774472</td>\n",
       "      <td>{'C': 10, 'class_weight': None, 'dual': False,...</td>\n",
       "      <td>0.762200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearSVC</th>\n",
       "      <td>0.773965</td>\n",
       "      <td>0.773965</td>\n",
       "      <td>{'C': 1, 'class_weight': None, 'dual': True, '...</td>\n",
       "      <td>0.761346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SGD</th>\n",
       "      <td>0.671075</td>\n",
       "      <td>0.776551</td>\n",
       "      <td>{'alpha': 1e-05, 'average': False, 'class_weig...</td>\n",
       "      <td>0.755837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MultinomialNB</th>\n",
       "      <td>0.677533</td>\n",
       "      <td>0.687875</td>\n",
       "      <td>{'alpha': 0.1, 'class_prior': None, 'fit_prior...</td>\n",
       "      <td>0.684142</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Результаты расчёта F1  F1 с гиперпарам.  \\\n",
       "LogisticRegression               0.748073          0.774472   \n",
       "LinearSVC                        0.773965          0.773965   \n",
       "SGD                              0.671075          0.776551   \n",
       "MultinomialNB                    0.677533          0.687875   \n",
       "\n",
       "                                                       Гиперпараметры  \\\n",
       "LogisticRegression  {'C': 10, 'class_weight': None, 'dual': False,...   \n",
       "LinearSVC           {'C': 1, 'class_weight': None, 'dual': True, '...   \n",
       "SGD                 {'alpha': 1e-05, 'average': False, 'class_weig...   \n",
       "MultinomialNB       {'alpha': 0.1, 'class_prior': None, 'fit_prior...   \n",
       "\n",
       "                    F1 тестовая  \n",
       "LogisticRegression     0.762200  \n",
       "LinearSVC              0.761346  \n",
       "SGD                    0.755837  \n",
       "MultinomialNB          0.684142  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод:**\n",
    "1. Обучил 4 модели машинного обучения;\n",
    "2. Результат, удовлетворяющий условию показали: LogisticRegression, LinearSVC;\n",
    "3. Лучший результат показала модель LogisticRegression.\n",
    "\n",
    "Bert не делаю, поскольку делаю на юпитере Яндекса все проекты, уже виснет ядро, но вроде не обязательно в задании."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Изучил и лемматизировал данные;\n",
    "2. Подготовил данные к машинному обучению;\n",
    "3. Обучил 4 модели классифицировать комментарии на позитивные и негативные;\n",
    "4. Наилучшие результаты показали 2 модели машинного обучения LogisticRegression, LinearSVC;\n",
    "5. Для решения поставленной в проекте задачи классификации комментариев стоит использовать LogisticRegression, поскольку она показала наилучшие результаты на всех этапах.\n",
    "6. Удалось добиться значения метрики F1 > 0.75\n",
    "\n",
    "В общем и целом обучил модель классифицировать комментарии на позитивные и негативные со значением метрики F1 больше 0,75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Чек-лист проверки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x]  Jupyter Notebook открыт\n",
    "- [x]  Весь код выполняется без ошибок\n",
    "- [x]  Ячейки с кодом расположены в порядке исполнения\n",
    "- [x]  Данные загружены и подготовлены\n",
    "- [x]  Модели обучены\n",
    "- [x]  Значение метрики *F1* не меньше 0.75\n",
    "- [x]  Выводы написаны"
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 2026,
    "start_time": "2022-05-18T13:21:27.852Z"
   },
   {
    "duration": 2536,
    "start_time": "2022-05-18T13:21:44.857Z"
   },
   {
    "duration": 33,
    "start_time": "2022-05-18T13:21:52.950Z"
   },
   {
    "duration": 14,
    "start_time": "2022-05-18T13:22:01.446Z"
   },
   {
    "duration": 30,
    "start_time": "2022-05-18T13:22:13.433Z"
   },
   {
    "duration": 131,
    "start_time": "2022-05-18T13:22:20.854Z"
   },
   {
    "duration": 240,
    "start_time": "2022-05-18T13:22:23.261Z"
   },
   {
    "duration": 10,
    "start_time": "2022-05-18T13:22:34.238Z"
   },
   {
    "duration": 11,
    "start_time": "2022-05-18T13:23:57.638Z"
   },
   {
    "duration": 3,
    "start_time": "2022-05-18T13:24:07.221Z"
   },
   {
    "duration": 13,
    "start_time": "2022-05-18T13:25:05.984Z"
   },
   {
    "duration": 12,
    "start_time": "2022-05-18T13:28:16.909Z"
   },
   {
    "duration": 45927,
    "start_time": "2022-05-18T13:28:35.272Z"
   },
   {
    "duration": 4,
    "start_time": "2022-05-18T13:29:55.038Z"
   },
   {
    "duration": 52,
    "start_time": "2022-05-18T13:29:58.154Z"
   },
   {
    "duration": 73,
    "start_time": "2022-05-18T13:30:05.581Z"
   },
   {
    "duration": 34,
    "start_time": "2022-05-18T13:30:09.090Z"
   },
   {
    "duration": 46,
    "start_time": "2022-05-18T13:30:36.812Z"
   },
   {
    "duration": 7,
    "start_time": "2022-05-18T13:30:45.691Z"
   },
   {
    "duration": 6,
    "start_time": "2022-05-18T13:30:55.213Z"
   },
   {
    "duration": 16,
    "start_time": "2022-05-18T13:31:07.528Z"
   },
   {
    "duration": 5559,
    "start_time": "2022-05-18T13:31:17.251Z"
   },
   {
    "duration": 6773,
    "start_time": "2022-05-18T13:31:42.640Z"
   },
   {
    "duration": 3,
    "start_time": "2022-05-18T13:31:57.941Z"
   },
   {
    "duration": 3,
    "start_time": "2022-05-18T13:32:41.964Z"
   },
   {
    "duration": 1817,
    "start_time": "2022-05-18T13:32:48.660Z"
   },
   {
    "duration": 946,
    "start_time": "2022-05-18T13:32:57.411Z"
   },
   {
    "duration": 272,
    "start_time": "2022-05-18T13:33:04.651Z"
   },
   {
    "duration": 57,
    "start_time": "2022-05-18T13:33:12.043Z"
   },
   {
    "duration": 10,
    "start_time": "2022-05-18T13:33:18.859Z"
   },
   {
    "duration": 4,
    "start_time": "2022-05-18T13:33:40.839Z"
   },
   {
    "duration": 13821,
    "start_time": "2022-05-18T13:33:52.326Z"
   },
   {
    "duration": 1895,
    "start_time": "2022-05-18T13:34:06.149Z"
   },
   {
    "duration": 100322,
    "start_time": "2022-05-18T13:34:09.540Z"
   },
   {
    "duration": 946,
    "start_time": "2022-05-18T13:35:49.864Z"
   },
   {
    "duration": 8805,
    "start_time": "2022-05-18T13:35:50.811Z"
   },
   {
    "duration": 616,
    "start_time": "2022-05-18T13:35:59.618Z"
   },
   {
    "duration": 771,
    "start_time": "2022-05-18T13:36:00.238Z"
   },
   {
    "duration": 70,
    "start_time": "2022-05-18T13:36:01.011Z"
   },
   {
    "duration": 11,
    "start_time": "2022-05-18T13:36:01.083Z"
   },
   {
    "duration": 1981,
    "start_time": "2022-05-18T13:36:01.096Z"
   },
   {
    "duration": 941,
    "start_time": "2022-05-18T13:36:03.078Z"
   },
   {
    "duration": 571,
    "start_time": "2022-05-18T13:36:04.021Z"
   },
   {
    "duration": 61,
    "start_time": "2022-05-18T13:36:04.594Z"
   },
   {
    "duration": 13,
    "start_time": "2022-05-18T13:36:04.657Z"
   },
   {
    "duration": 3,
    "start_time": "2022-05-18T13:36:07.298Z"
   },
   {
    "duration": 10,
    "start_time": "2022-05-18T13:36:17.796Z"
   },
   {
    "duration": 12,
    "start_time": "2022-05-18T13:36:58.476Z"
   },
   {
    "duration": 3,
    "start_time": "2022-05-18T13:37:02.061Z"
   },
   {
    "duration": 15480,
    "start_time": "2022-05-18T13:37:09.868Z"
   },
   {
    "duration": 5,
    "start_time": "2022-05-18T13:37:26.500Z"
   },
   {
    "duration": 979,
    "start_time": "2022-05-18T13:37:36.581Z"
   },
   {
    "duration": 27,
    "start_time": "2022-05-18T13:37:44.782Z"
   },
   {
    "duration": 5,
    "start_time": "2022-05-18T13:37:55.468Z"
   },
   {
    "duration": 2428,
    "start_time": "2022-05-18T14:03:56.513Z"
   },
   {
    "duration": 3523,
    "start_time": "2022-05-18T14:03:58.943Z"
   },
   {
    "duration": 37,
    "start_time": "2022-05-18T14:04:02.467Z"
   },
   {
    "duration": 15,
    "start_time": "2022-05-18T14:04:02.509Z"
   },
   {
    "duration": 25,
    "start_time": "2022-05-18T14:04:02.526Z"
   },
   {
    "duration": 253,
    "start_time": "2022-05-18T14:04:02.553Z"
   },
   {
    "duration": 10,
    "start_time": "2022-05-18T14:04:02.809Z"
   },
   {
    "duration": 7,
    "start_time": "2022-05-18T14:04:02.821Z"
   },
   {
    "duration": 48774,
    "start_time": "2022-05-18T14:04:02.830Z"
   },
   {
    "duration": 4,
    "start_time": "2022-05-18T14:04:51.607Z"
   },
   {
    "duration": 64,
    "start_time": "2022-05-18T14:04:51.613Z"
   },
   {
    "duration": 42,
    "start_time": "2022-05-18T14:04:51.679Z"
   },
   {
    "duration": 40,
    "start_time": "2022-05-18T14:04:51.723Z"
   },
   {
    "duration": 6,
    "start_time": "2022-05-18T14:04:51.765Z"
   },
   {
    "duration": 10,
    "start_time": "2022-05-18T14:04:51.773Z"
   },
   {
    "duration": 10,
    "start_time": "2022-05-18T14:04:51.785Z"
   },
   {
    "duration": 5360,
    "start_time": "2022-05-18T14:04:51.801Z"
   },
   {
    "duration": 6911,
    "start_time": "2022-05-18T14:04:57.163Z"
   },
   {
    "duration": 5,
    "start_time": "2022-05-18T14:05:04.076Z"
   },
   {
    "duration": 1862,
    "start_time": "2022-05-18T14:05:04.083Z"
   },
   {
    "duration": 983,
    "start_time": "2022-05-18T14:05:05.947Z"
   },
   {
    "duration": 284,
    "start_time": "2022-05-18T14:05:06.932Z"
   },
   {
    "duration": 52,
    "start_time": "2022-05-18T14:05:07.217Z"
   },
   {
    "duration": 29,
    "start_time": "2022-05-18T14:05:07.270Z"
   },
   {
    "duration": 34,
    "start_time": "2022-05-18T14:05:07.301Z"
   },
   {
    "duration": 12798,
    "start_time": "2022-05-18T14:05:07.337Z"
   },
   {
    "duration": 1778,
    "start_time": "2022-05-18T14:05:20.137Z"
   },
   {
    "duration": 105001,
    "start_time": "2022-05-18T14:05:21.919Z"
   },
   {
    "duration": 1061,
    "start_time": "2022-05-18T14:07:06.922Z"
   },
   {
    "duration": 8018,
    "start_time": "2022-05-18T14:07:07.984Z"
   },
   {
    "duration": 565,
    "start_time": "2022-05-18T14:07:16.004Z"
   },
   {
    "duration": 742,
    "start_time": "2022-05-18T14:07:16.571Z"
   },
   {
    "duration": 53,
    "start_time": "2022-05-18T14:07:17.315Z"
   },
   {
    "duration": 27,
    "start_time": "2022-05-18T14:07:17.370Z"
   },
   {
    "duration": 1832,
    "start_time": "2022-05-18T14:07:17.400Z"
   },
   {
    "duration": 1019,
    "start_time": "2022-05-18T14:07:19.234Z"
   },
   {
    "duration": 530,
    "start_time": "2022-05-18T14:07:20.254Z"
   },
   {
    "duration": 47,
    "start_time": "2022-05-18T14:07:20.786Z"
   },
   {
    "duration": 8,
    "start_time": "2022-05-18T14:07:20.834Z"
   },
   {
    "duration": 22,
    "start_time": "2022-05-18T14:07:20.844Z"
   },
   {
    "duration": 29,
    "start_time": "2022-05-18T14:07:20.867Z"
   },
   {
    "duration": 1718,
    "start_time": "2022-05-20T15:42:12.666Z"
   },
   {
    "duration": 2413,
    "start_time": "2022-05-20T15:42:14.387Z"
   },
   {
    "duration": 29,
    "start_time": "2022-05-20T15:42:16.801Z"
   },
   {
    "duration": 14,
    "start_time": "2022-05-20T15:42:16.832Z"
   },
   {
    "duration": 49415,
    "start_time": "2022-05-20T15:44:50.541Z"
   },
   {
    "duration": 36,
    "start_time": "2022-05-20T15:46:11.658Z"
   },
   {
    "duration": 251,
    "start_time": "2022-05-20T15:46:12.137Z"
   },
   {
    "duration": 6,
    "start_time": "2022-05-20T15:46:12.536Z"
   },
   {
    "duration": 25,
    "start_time": "2022-05-20T15:46:13.352Z"
   },
   {
    "duration": 46564,
    "start_time": "2022-05-20T15:46:13.837Z"
   },
   {
    "duration": 4,
    "start_time": "2022-05-20T15:47:00.403Z"
   },
   {
    "duration": 80,
    "start_time": "2022-05-20T15:47:00.409Z"
   },
   {
    "duration": 19,
    "start_time": "2022-05-20T15:47:13.153Z"
   },
   {
    "duration": 123,
    "start_time": "2022-05-20T15:47:26.978Z"
   },
   {
    "duration": 9,
    "start_time": "2022-05-20T15:47:36.642Z"
   },
   {
    "duration": 702,
    "start_time": "2022-05-20T15:48:05.621Z"
   },
   {
    "duration": 14,
    "start_time": "2022-05-20T15:48:15.043Z"
   },
   {
    "duration": 95,
    "start_time": "2022-05-20T15:49:29.179Z"
   },
   {
    "duration": 67,
    "start_time": "2022-05-20T15:49:49.843Z"
   },
   {
    "duration": 43,
    "start_time": "2022-05-20T15:49:50.641Z"
   },
   {
    "duration": 28,
    "start_time": "2022-05-20T15:51:27.930Z"
   },
   {
    "duration": 64,
    "start_time": "2022-05-20T17:06:08.066Z"
   },
   {
    "duration": 12,
    "start_time": "2022-05-20T17:40:32.364Z"
   },
   {
    "duration": 1351,
    "start_time": "2022-05-20T17:41:32.518Z"
   },
   {
    "duration": 894,
    "start_time": "2022-05-20T17:41:33.871Z"
   },
   {
    "duration": 33,
    "start_time": "2022-05-20T17:41:34.767Z"
   },
   {
    "duration": 14,
    "start_time": "2022-05-20T17:41:34.802Z"
   },
   {
    "duration": 56,
    "start_time": "2022-05-20T17:41:34.819Z"
   },
   {
    "duration": 248,
    "start_time": "2022-05-20T17:41:34.877Z"
   },
   {
    "duration": 7,
    "start_time": "2022-05-20T17:41:35.126Z"
   },
   {
    "duration": 46041,
    "start_time": "2022-05-20T17:41:35.134Z"
   },
   {
    "duration": 4,
    "start_time": "2022-05-20T17:42:21.178Z"
   },
   {
    "duration": 96,
    "start_time": "2022-05-20T17:42:21.184Z"
   },
   {
    "duration": 33,
    "start_time": "2022-05-20T17:42:21.282Z"
   },
   {
    "duration": 51,
    "start_time": "2022-05-20T17:42:21.317Z"
   },
   {
    "duration": 6,
    "start_time": "2022-05-20T17:42:21.370Z"
   },
   {
    "duration": 7,
    "start_time": "2022-05-20T17:42:21.379Z"
   },
   {
    "duration": 8,
    "start_time": "2022-05-20T17:42:21.387Z"
   },
   {
    "duration": 5252,
    "start_time": "2022-05-20T17:42:21.397Z"
   },
   {
    "duration": 6830,
    "start_time": "2022-05-20T17:42:26.650Z"
   },
   {
    "duration": 3,
    "start_time": "2022-05-20T17:42:33.482Z"
   },
   {
    "duration": 1756,
    "start_time": "2022-05-20T17:42:33.486Z"
   },
   {
    "duration": 897,
    "start_time": "2022-05-20T17:42:35.244Z"
   },
   {
    "duration": 278,
    "start_time": "2022-05-20T17:42:36.143Z"
   },
   {
    "duration": 60,
    "start_time": "2022-05-20T17:42:36.423Z"
   },
   {
    "duration": 8,
    "start_time": "2022-05-20T17:42:36.485Z"
   },
   {
    "duration": 6,
    "start_time": "2022-05-20T17:42:36.495Z"
   },
   {
    "duration": 13007,
    "start_time": "2022-05-20T17:42:36.503Z"
   },
   {
    "duration": 1835,
    "start_time": "2022-05-20T17:42:49.512Z"
   },
   {
    "duration": 66011,
    "start_time": "2022-05-20T17:42:51.349Z"
   },
   {
    "duration": 0,
    "start_time": "2022-05-20T17:43:57.362Z"
   },
   {
    "duration": 0,
    "start_time": "2022-05-20T17:43:57.363Z"
   },
   {
    "duration": 0,
    "start_time": "2022-05-20T17:43:57.364Z"
   },
   {
    "duration": 0,
    "start_time": "2022-05-20T17:43:57.365Z"
   },
   {
    "duration": 0,
    "start_time": "2022-05-20T17:43:57.366Z"
   },
   {
    "duration": 0,
    "start_time": "2022-05-20T17:43:57.367Z"
   },
   {
    "duration": 0,
    "start_time": "2022-05-20T17:43:57.368Z"
   },
   {
    "duration": 0,
    "start_time": "2022-05-20T17:43:57.369Z"
   },
   {
    "duration": 0,
    "start_time": "2022-05-20T17:43:57.371Z"
   },
   {
    "duration": 0,
    "start_time": "2022-05-20T17:43:57.372Z"
   },
   {
    "duration": 0,
    "start_time": "2022-05-20T17:43:57.372Z"
   },
   {
    "duration": 0,
    "start_time": "2022-05-20T17:43:57.373Z"
   },
   {
    "duration": 0,
    "start_time": "2022-05-20T17:43:57.374Z"
   },
   {
    "duration": 112027,
    "start_time": "2022-05-20T17:44:00.109Z"
   },
   {
    "duration": 845,
    "start_time": "2022-05-20T17:45:56.640Z"
   },
   {
    "duration": 8050,
    "start_time": "2022-05-20T17:45:58.438Z"
   },
   {
    "duration": 528,
    "start_time": "2022-05-20T17:46:09.862Z"
   },
   {
    "duration": 730,
    "start_time": "2022-05-20T17:46:13.127Z"
   },
   {
    "duration": 56,
    "start_time": "2022-05-20T17:46:14.918Z"
   },
   {
    "duration": 11,
    "start_time": "2022-05-20T17:46:16.678Z"
   },
   {
    "duration": 1740,
    "start_time": "2022-05-20T17:46:22.334Z"
   },
   {
    "duration": 774,
    "start_time": "2022-05-20T17:46:24.838Z"
   },
   {
    "duration": 504,
    "start_time": "2022-05-20T17:46:25.614Z"
   },
   {
    "duration": 47,
    "start_time": "2022-05-20T17:46:26.120Z"
   },
   {
    "duration": 9,
    "start_time": "2022-05-20T17:46:26.438Z"
   },
   {
    "duration": 4,
    "start_time": "2022-05-20T17:46:30.159Z"
   },
   {
    "duration": 13,
    "start_time": "2022-05-20T17:46:30.622Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
